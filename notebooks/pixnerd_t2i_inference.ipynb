{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PixNerd text-to-image checkpoint inference (T2I)\n",
        "\n",
        "This notebook loads a PixNerd XL text-to-image checkpoint, runs prompt-based sampling, and visualizes both the training resolution and higher-resolution outputs from within the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment & paths\n",
        "- Requires a GPU runtime (the Qwen3 text encoder forces CUDA and bfloat16).\n",
        "- Assumes dependencies from `requirements.txt` are installed.\n",
        "- Place the checkpoint at `checkpoints/PixNerd-XL-P16-C2I/model.ckpt` relative to the repo root (adjust the path if yours differs).\n",
        "- Set `TEXT_ENCODER_PATH` to a local or cached copy of `Qwen/Qwen3-1.7B` if your environment lacks internet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import math\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Optional: change to your repo root if running from elsewhere\n",
        "REPO_ROOT = Path(os.getcwd())\n",
        "\n",
        "# Checkpoint location (adjust if yours lives elsewhere)\n",
        "CKPT_PATH = REPO_ROOT / \"checkpoints/PixNerd-XL-P16-C2I/model.ckpt\"\n",
        "\n",
        "# Text encoder weights (Hugging Face model name or local path)\n",
        "TEXT_ENCODER_PATH = \"Qwen/Qwen3-1.7B\"\n",
        "\n",
        "# Where to save generated images\n",
        "OUTPUT_DIR = REPO_ROOT / \"notebooks\" / \"pixnerd_t2i_inference_outputs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if DEVICE != \"cuda\":\n",
        "    raise RuntimeError(\"PixNerd T2I inference requires a CUDA GPU (Qwen3TextEncoder uses .cuda())\")\n",
        "\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"Checkpoint exists: {CKPT_PATH.exists()}\")\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the PixNerd XL T2I model components\n",
        "This mirrors the text-to-image configuration:\n",
        "- Pixel-space autoencoder (`PixelAE`)\n",
        "- Qwen3 text conditioner\n",
        "- PixNerDiT heavy decoder (patch size 16, hidden dim 1536)\n",
        "- Adam LM sampler with classifier-free guidance\n",
        "- REPATrainer stub (only so projection weights load from the checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.models.autoencoder.pixel import PixelAE\n",
        "from src.models.conditioner.qwen3_text_encoder import Qwen3TextEncoder\n",
        "from src.models.transformer.pixnerd_t2i_heavydecoder import PixNerDiT\n",
        "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
        "from src.diffusion.flow_matching.adam_sampling import AdamLMSampler, ode_step_fn\n",
        "from src.diffusion.base.guidance import simple_guidance_fn\n",
        "from src.diffusion.flow_matching.training_repa import REPATrainer\n",
        "from src.callbacks.simple_ema import SimpleEMA\n",
        "from src.lightning_model import LightningModel\n",
        "from src.models.encoder import IndentityMapping\n",
        "from src.models.autoencoder.base import fp2uint8\n",
        "\n",
        "# Model hyperparameters (from configs_t2i/inference_heavydecoder.yaml)\n",
        "HIDDEN_SIZE = 1536\n",
        "TXT_EMBED_DIM = 2048\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "# Scheduler\n",
        "main_scheduler = LinearScheduler()\n",
        "\n",
        "# Core modules\n",
        "vae = PixelAE(scale=1.0)\n",
        "conditioner = Qwen3TextEncoder(weight_path=TEXT_ENCODER_PATH, embed_dim=TXT_EMBED_DIM, max_length=128)\n",
        "denoiser = PixNerDiT(\n",
        "    in_channels=3,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_groups=24,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    txt_embed_dim=TXT_EMBED_DIM,\n",
        "    txt_max_length=128,\n",
        "    num_text_blocks=4,\n",
        "    decoder_hidden_size=64,\n",
        "    num_encoder_blocks=16,\n",
        "    num_decoder_blocks=2,\n",
        ")\n",
        "\n",
        "# Sampler mirrors the text-to-image setup\n",
        "sampler = AdamLMSampler(\n",
        "    num_steps=25,\n",
        "    guidance=4.0,\n",
        "    timeshift=3.0,\n",
        "    order=2,\n",
        "    scheduler=main_scheduler,\n",
        "    guidance_fn=simple_guidance_fn,\n",
        "    step_fn=ode_step_fn,\n",
        ")\n",
        "\n",
        "# REPATrainer is only instantiated so its projection MLP weights can be loaded from the checkpoint\n",
        "trainer_stub = REPATrainer(\n",
        "    scheduler=main_scheduler,\n",
        "    lognorm_t=True,\n",
        "    timeshift=4.0,\n",
        "    feat_loss_weight=0.5,\n",
        "    encoder=IndentityMapping(),  # placeholder; encoder is unused for inference\n",
        "    align_layer=6,\n",
        "    proj_denoiser_dim=HIDDEN_SIZE,\n",
        "    proj_hidden_dim=HIDDEN_SIZE,\n",
        "    proj_encoder_dim=768,\n",
        ")\n",
        "\n",
        "ema_tracker = SimpleEMA(decay=0.9999)\n",
        "\n",
        "# Wrap everything in the LightningModel for easy checkpoint loading\n",
        "model = LightningModel(\n",
        "    vae=vae,\n",
        "    conditioner=conditioner,\n",
        "    denoiser=denoiser,\n",
        "    diffusion_trainer=trainer_stub,\n",
        "    diffusion_sampler=sampler,\n",
        "    ema_tracker=ema_tracker,\n",
        "    optimizer=None,\n",
        "    lr_scheduler=None,\n",
        "    eval_original_model=False,\n",
        ")\n",
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "print(\"Model initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the checkpoint (EMA weights)\n",
        "`strict=False` lets us ignore any optimizer or trainer state that may be absent. EMA weights are stored under `ema_denoiser.*` and are used for sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "missing, unexpected = model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
        "print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n",
        "print(\"Ready for text-to-image sampling with EMA denoiser.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling helpers\n",
        "- `sample_prompts` draws Gaussian noise, runs the Adam LM sampler with classifier-free guidance, decodes to pixel space, and returns uint8 tensors.\n",
        "- `save_grid` writes a simple grid PNG for quick inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_prompts(prompts, height=512, width=512, seed=0, guidance=4.0, num_steps=25):\n",
        "    torch.manual_seed(seed)\n",
        "    batch = len(prompts)\n",
        "\n",
        "    # Update guidance or steps on the fly if desired\n",
        "    model.diffusion_sampler.guidance = guidance\n",
        "    model.diffusion_sampler.num_steps = num_steps\n",
        "\n",
        "    noise = torch.randn(batch, 3, height, width, device=DEVICE)\n",
        "\n",
        "    condition, uncondition = model.conditioner(prompts)\n",
        "    samples = model.diffusion_sampler(\n",
        "        model.ema_denoiser,\n",
        "        noise,\n",
        "        condition,\n",
        "        uncondition,\n",
        "    )\n",
        "    images = model.vae.decode(samples)\n",
        "    images = torch.clamp(images, -1.0, 1.0)\n",
        "    images_uint8 = fp2uint8(images)\n",
        "    return images_uint8.cpu()\n",
        "\n",
        "\n",
        "def save_grid(images_uint8, filename, cols=None):\n",
        "    imgs = [Image.fromarray(img) for img in images_uint8]\n",
        "    n = len(imgs)\n",
        "    if cols is None:\n",
        "        cols = math.ceil(math.sqrt(n))\n",
        "    rows = math.ceil(n / cols)\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", (cols * w, rows * h))\n",
        "    for idx, img in enumerate(imgs):\n",
        "        r, c = divmod(idx, cols)\n",
        "        grid.paste(img, (c * w, r * h))\n",
        "    out_path = OUTPUT_DIR / filename\n",
        "    grid.save(out_path)\n",
        "    return out_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate 512\u00d7512 samples from prompts\n",
        "Edit `PROMPTS` to whatever you like. Adjust `SEED`, `GUIDANCE`, or `NUM_STEPS` as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPTS = [\n",
        "    \"a photo of a red fox in a snowy forest\",\n",
        "    \"a futuristic city skyline at sunset, digital art\",\n",
        "    \"a cozy reading nook with warm lighting\",\n",
        "]\n",
        "SEED = 123\n",
        "GUIDANCE = 4.0\n",
        "NUM_STEPS = 25\n",
        "\n",
        "images_512 = sample_prompts(\n",
        "    prompts=PROMPTS,\n",
        "    height=512,\n",
        "    width=512,\n",
        "    seed=SEED,\n",
        "    guidance=GUIDANCE,\n",
        "    num_steps=NUM_STEPS,\n",
        ")\n",
        "\n",
        "for i, img in enumerate(images_512):\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Prompt {i}\")\n",
        "plt.show()\n",
        "\n",
        "save_grid(images_512, \"pixnerd_t2i_512_grid.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Super-resolution sampling (e.g., 768\u00d7768 or 1024\u00d71024)\n",
        "Because the model works in pixel space with coordinate-aware embeddings, simply draw higher-resolution noise and run the same sampler. You can tweak guidance/steps for the higher resolution if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HIGH_RES = 768  # change to any square size your GPU supports\n",
        "SUPERRES_SEED = 7\n",
        "SUPERRES_GUIDANCE = 4.5\n",
        "SUPERRES_STEPS = 40\n",
        "\n",
        "images_sr = sample_prompts(\n",
        "    prompts=PROMPTS,\n",
        "    height=HIGH_RES,\n",
        "    width=HIGH_RES,\n",
        "    seed=SUPERRES_SEED,\n",
        "    guidance=SUPERRES_GUIDANCE,\n",
        "    num_steps=SUPERRES_STEPS,\n",
        ")\n",
        "\n",
        "for i, img in enumerate(images_sr):\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Prompt {i} @ {HIGH_RES}x{HIGH_RES}\")\n",
        "plt.show()\n",
        "\n",
        "save_grid(images_sr, f\"pixnerd_t2i_{HIGH_RES}sq_grid.png\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}