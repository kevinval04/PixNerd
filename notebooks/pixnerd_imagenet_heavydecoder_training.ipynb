{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PixNerd ImageNet-256 Training (Heavy Decoder, Class-Conditional)\n",
    "\n",
    "This self-contained notebook trains PixNerd on ImageNet using the **text-to-image heavy decoder** as a neural-field head while keeping diffusion on the 256\u00d7256 training grid. It loads images from `/pscratch/sd/k/kevinval/datasets/imagenet256` (class-folder layout) and uses class embeddings instead of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "- Requires CUDA for meaningful speed.\n",
    "- Imports modules directly from the repository source tree (no Lightning CLI).\n",
    "- Adjust batch size/epochs to fit your GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, random, time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: ImageNet-256 loaders\n",
    "Images are normalized to [-1, 1] for diffusion training. The dataset path should contain class subfolders with JPEGs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and hyperparameters\n",
    "IMAGENET_ROOT = \"/pscratch/sd/k/kevinval/datasets/imagenet256\"\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "NUM_CLASSES = 1000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=IMAGENET_ROOT,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print('Train images:', len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise schedule helpers (DDPM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(T=1000, beta_start=0.0001, beta_end=0.02):\n",
    "    return torch.linspace(beta_start, beta_end, T)\n",
    "\n",
    "@dataclass\n",
    "class DiffusionSchedule:\n",
    "    betas: torch.Tensor\n",
    "    alphas: torch.Tensor\n",
    "    alpha_bars: torch.Tensor\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, T=1000, beta_start=0.0001, beta_end=0.02, device='cpu'):\n",
    "        betas = make_beta_schedule(T, beta_start, beta_end).to(device)\n",
    "        alphas = 1.0 - betas\n",
    "        alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "        return cls(betas, alphas, alpha_bars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy decoder wrapper for class conditioning\n",
    "We reuse the text-to-image heavy decoder and feed it with learned class embeddings (length 1 sequence). Coordinate interpolation for super-resolution is controlled via `decoder_patch_scaling_*`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.transformer.pixnerd_t2i_heavydecoder import PixNerDiT\n",
    "\n",
    "class ClassConditionalPixNerDiT(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, in_channels=3, patch_size=2, hidden_size=1152,\n",
    "                 decoder_hidden_size=64, num_encoder_blocks=18, num_decoder_blocks=4, num_text_blocks=4,\n",
    "                 txt_embed_dim=1024, txt_max_length=1):\n",
    "        super().__init__()\n",
    "        self.model = PixNerDiT(\n",
    "            in_channels=in_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            decoder_hidden_size=decoder_hidden_size,\n",
    "            num_encoder_blocks=num_encoder_blocks,\n",
    "            num_decoder_blocks=num_decoder_blocks,\n",
    "            num_text_blocks=num_text_blocks,\n",
    "            patch_size=patch_size,\n",
    "            txt_embed_dim=txt_embed_dim,\n",
    "            txt_max_length=txt_max_length,\n",
    "        )\n",
    "        self.class_embed = nn.Embedding(num_classes, txt_embed_dim)\n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        y = self.class_embed(labels).unsqueeze(1)\n",
    "        return self.model(x, t, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion model tying everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixNerdDDPM(nn.Module):\n",
    "    def __init__(self, model: ClassConditionalPixNerDiT, schedule: DiffusionSchedule):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def loss(self, x0, labels):\n",
    "        b = x0.size(0)\n",
    "        t = torch.randint(0, self.schedule.betas.size(0), (b,), device=x0.device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        alpha_bar_t = self.schedule.alpha_bars[t].view(-1, 1, 1, 1)\n",
    "        xt = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "        pred = self.model(xt, t, labels)\n",
    "        return F.mse_loss(pred, noise)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, labels, timesteps=50, img_size=IMAGE_SIZE):\n",
    "        x = torch.randn(batch_size, 3, img_size, img_size, device=labels.device)\n",
    "        T = self.schedule.betas.size(0)\n",
    "        skip = T // timesteps\n",
    "        for i in range(T - 1, -1, -skip):\n",
    "            t = torch.full((batch_size,), i, device=labels.device, dtype=torch.long)\n",
    "            beta_t = self.schedule.betas[t].view(-1, 1, 1, 1)\n",
    "            alpha_t = self.schedule.alphas[t].view(-1, 1, 1, 1)\n",
    "            alpha_bar_t = self.schedule.alpha_bars[t].view(-1, 1, 1, 1)\n",
    "            eps = self.model(x, t, labels)\n",
    "            x0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps) / torch.sqrt(alpha_bar_t)\n",
    "            if i > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = 0\n",
    "            x = torch.sqrt(alpha_t) * x0_pred + torch.sqrt(beta_t) * noise\n",
    "        return x.clamp(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model, optimizer, EMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = DiffusionSchedule.create(T=1000, device=device)\n",
    "model = ClassConditionalPixNerDiT(num_classes=NUM_CLASSES).to(device)\n",
    "diffusion = PixNerdDDPM(model, schedule).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(diffusion.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "# Simple EMA for stability\n",
    "ema_decay = 0.999\n",
    "ema_diffusion = PixNerdDDPM(ClassConditionalPixNerDiT(num_classes=NUM_CLASSES).to(device), schedule)\n",
    "ema_diffusion.load_state_dict(diffusion.state_dict())\n",
    "\n",
    "def update_ema(target, source, decay):\n",
    "    with torch.no_grad():\n",
    "        for tgt, src in zip(target.parameters(), source.parameters()):\n",
    "            tgt.data.mul_(decay).add_(src.data, alpha=1 - decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Minimal loop; increase epochs/steps for real training. Add checkpointing/logging as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1   # increase to train properly\n",
    "log_interval = 50\n",
    "save_dir = Path(\"./pixnerd_imagenet_heavy_ckpts\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    diffusion.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        loss = diffusion.loss(imgs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        update_ema(ema_diffusion, diffusion, ema_decay)\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print(f\"epoch {epoch} step {step} loss {loss.item():.4f}\")\n",
    "        step += 1\n",
    "\n",
    "    torch.save({\n",
    "        'diffusion': diffusion.state_dict(),\n",
    "        'ema_diffusion': ema_diffusion.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "    }, save_dir / f\"epoch{epoch:03d}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-resolution sampling\n",
    "Run the heavy decoder at a higher resolution (e.g., 512\u00d7512) while keeping the diffusion grid at 256\u00d7256.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_superres(num_imgs=4, target_size=(512, 512), timesteps=50):\n",
    "    ema_diffusion.eval()\n",
    "    h, w = target_size\n",
    "    labels = torch.randint(0, NUM_CLASSES, (num_imgs,), device=device)\n",
    "\n",
    "    # Base sampling on the training grid\n",
    "    base = ema_diffusion.sample(batch_size=num_imgs, labels=labels, timesteps=timesteps, img_size=IMAGE_SIZE)\n",
    "\n",
    "    # Scale decoder patches for super-resolution\n",
    "    scale_h = h / IMAGE_SIZE\n",
    "    scale_w = w / IMAGE_SIZE\n",
    "    model.model.decoder_patch_scaling_h = scale_h\n",
    "    model.model.decoder_patch_scaling_w = scale_w\n",
    "    ema_diffusion.model.model.decoder_patch_scaling_h = scale_h\n",
    "    ema_diffusion.model.model.decoder_patch_scaling_w = scale_w\n",
    "\n",
    "    up = torch.nn.functional.interpolate(base, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    refined = ema_diffusion.model(up, torch.zeros(num_imgs, device=device, dtype=torch.long), labels)\n",
    "\n",
    "    # Restore default scaling\n",
    "    model.model.decoder_patch_scaling_h = 1.0\n",
    "    model.model.decoder_patch_scaling_w = 1.0\n",
    "    ema_diffusion.model.model.decoder_patch_scaling_h = 1.0\n",
    "    ema_diffusion.model.model.decoder_patch_scaling_w = 1.0\n",
    "\n",
    "    imgs = (refined.clamp(-1, 1) + 1) * 0.5\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=2)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example (commented to avoid accidental long runs)\n",
    "# sample_superres(num_imgs=4, target_size=(512, 512), timesteps=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}