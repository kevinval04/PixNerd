{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PixNerd ImageNet Training & Super-Resolution Notebook\n\n",
        "This notebook mirrors the repository's standard Lightning-CLI workflow for PixNerd: install dependencies, create a run-ready config for ImageNet training, launch fitting, and generate higher-resolution samples from a trained checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "- A multi-GPU machine with enough memory to match the original PixNerd-XL ImageNet runs (the default batch sizes assume multiple high-memory GPUs).\n",
        "- A local ImageNet-1K training set (folder structure compatible with `torchvision.datasets.ImageFolder`).\n",
        "- Access to the DINOv2 ViT-B/14 weights used for the feature-alignment loss.\n",
        "- (Optional) Weights & Biases credentials if you want online logging; otherwise you can disable the logger.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you haven't installed dependencies in this environment, uncomment the next line.\n",
        "# !pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "\n",
        "# Paths you need to customize\n",
        "IMAGENET_ROOT = \"/path/to/imagenet/train\"          # ImageNet training folder\n",
        "DINOV2_WEIGHTS = \"/path/to/dinov2_vitb14\"          # Local DINOv2 ViT-B/14 checkpoint directory\n",
        "WORKDIR_ROOT = Path(\"./workdirs_notebook\")          # Where logs, checkpoints, and samples will be saved\n",
        "\n",
        "EXP_NAME = f\"pixnerd_imagenet256_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "BASE_CONFIG = Path(\"configs_c2i/pix256std1_repa_pixnerd_xl.yaml\")\n",
        "\n",
        "cfg = OmegaConf.load(BASE_CONFIG)\n",
        "\n",
        "# Override training/logging paths to local locations\n",
        "cfg.tags.exp = EXP_NAME\n",
        "cfg.trainer.default_root_dir = str(WORKDIR_ROOT)\n",
        "if cfg.trainer.logger:\n",
        "    cfg.trainer.logger.init_args.name = EXP_NAME\n",
        "    # Set your own project if desired; keep the default to match the paper runs\n",
        "    cfg.trainer.logger.init_args.project = cfg.trainer.logger.init_args.get(\"project\", \"universal_pix_flow\")\n",
        "\n",
        "# Point datasets and frozen encoder to your local data\n",
        "cfg.data.train_dataset.init_args.root = IMAGENET_ROOT\n",
        "cfg.data.train_dataset.init_args.resolution = 256\n",
        "cfg.data.eval_dataset.init_args.latent_shape = [3, 256, 256]\n",
        "cfg.data.pred_dataset.init_args.latent_shape = [3, 256, 256]\n",
        "cfg.diffusion_trainer.init_args.encoder.init_args.weight_path = DINOV2_WEIGHTS\n",
        "\n",
        "# Cache overrides are optional; keep them None unless you need a custom hub path\n",
        "cfg.torch_hub_dir = None\n",
        "cfg.huggingface_cache_dir = None\n",
        "\n",
        "# Save the run-specific config next to this notebook\n",
        "notebook_cfg = Path(\"notebooks/configs/pixnerd_imagenet256_notebook.yaml\")\n",
        "notebook_cfg.parent.mkdir(parents=True, exist_ok=True)\n",
        "OmegaConf.save(cfg, notebook_cfg)\n",
        "print(\"Saved config to\", notebook_cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The saved config mirrors `configs_c2i/pix256std1_repa_pixnerd_xl.yaml` but with local paths/tags. Training outputs land in `WORKDIR_ROOT/exp_<EXP_NAME>/...`, and the `SaveImagesHook` writes validation/prediction images under the same root.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect a subset of the resolved config (feel free to delete or extend)\n",
        "print(OmegaConf.to_yaml(cfg, resolve=True)[:2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch full PixNerd ImageNet training\n\n",
        "This cell runs the exact Lightning-CLI entrypoint used in the repository. It will respect multi-GPU settings from the config (mixed precision, EMA, checkpointing, periodic validation saves). Expect long runtimes similar to the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Starts training; comment this out if you only want inference with a pre-trained checkpoint.\n",
        "# !python main.py fit -c notebooks/configs/pixnerd_imagenet256_notebook.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, Lightning stores checkpoints in `WORKDIR_ROOT/exp_<EXP_NAME>/checkpoints/`. Replace `CKPT_PATH` below with `last.ckpt` or the best-step checkpoint you want to sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "RUN_DIR = Path(cfg.trainer.default_root_dir) / f\"exp_{cfg.tags.exp}\"\n",
        "CKPT_PATH = RUN_DIR / \"checkpoints/last.ckpt\"  # update if you prefer a specific step\n",
        "print(\"Expected checkpoint:\", CKPT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample at training resolution (256\u00d7256)\n\n",
        "Use the `predict` subcommand to generate class-conditional samples at the training size. Outputs are written to `<default_root_dir>/<save_dir>/predict/` (with `save_dir` coming from `SaveImagesHook`, default `val`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python main.py predict -c notebooks/configs/pixnerd_imagenet256_notebook.yaml --ckpt_path $CKPT_PATH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Super-resolution sampling (e.g., 512\u00d7512)\n\n",
        "PixNerd can sample larger grids without architecture changes. Clone the saved config, bump the latent shape to the target resolution, and lower the batch size if needed. The same checkpoint is reused.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "superres_cfg = OmegaConf.load(notebook_cfg)\n",
        "superres_cfg.data.pred_dataset.init_args.latent_shape = [3, 512, 512]\n",
        "superres_cfg.data.pred_batch_size = 4  # adjust for your memory budget\n",
        "superres_cfg.trainer.default_root_dir = str(Path(cfg.trainer.default_root_dir) / \"superres_runs\")\n",
        "\n",
        "superres_cfg_path = Path(\"notebooks/configs/pixnerd_imagenet512_predict.yaml\")\n",
        "OmegaConf.save(superres_cfg, superres_cfg_path)\n",
        "print(\"Super-resolution config saved to\", superres_cfg_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python main.py predict -c notebooks/configs/pixnerd_imagenet512_predict.yaml --ckpt_path $CKPT_PATH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize predicted images\n\n",
        "The `SaveImagesHook` writes PNGs (first few samples) and a compressed `.npz` array. Update `PRED_DIR` to point at the prediction folder for either the 256\u00d7256 or super-resolution run and visualize a handful of images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Point this to the prediction directory you want to browse\n",
        "PRED_DIR = Path(superres_cfg.trainer.default_root_dir) / superres_cfg.trainer.callbacks[1].init_args.save_dir / \"predict\"\n",
        "print(\"Reading from\", PRED_DIR)\n",
        "\n",
        "images = sorted(PRED_DIR.glob(\"*.png\"))[:8]\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(4*len(images), 4))\n",
        "for ax, image_path in zip(axes, images):\n",
        "    ax.imshow(Image.open(image_path))\n",
        "    ax.set_title(image_path.name)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}