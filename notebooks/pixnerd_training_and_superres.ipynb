{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PixNerd ImageNet Training & Super-Resolution (No Lightning CLI)\n",
    "\n",
    "This self-contained notebook trains and samples PixNerd directly from Python objects\u2014no Lightning CLI invocations required. It mirrors the original configuration while keeping everything editable inside the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- A multi-GPU machine with enough memory for PixNerd-XL ImageNet training.\n",
    "- Local ImageNet-1K training data (folder layout compatible with `torchvision.datasets.ImageFolder`).\n",
    "- Local DINOv2 ViT-B/14 weights for the feature-alignment loss.\n",
    "- (Optional) Weights & Biases credentials if you want online logging."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If dependencies are not installed in this environment, uncomment the next line.\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from lightning import Trainer, seed_everything\n",
    "import lightning.pytorch as pl\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.lightning_data import DataModule\n",
    "from src.lightning_model import LightningModel\n",
    "\n",
    "\n",
    "def _resolve_target(target):\n",
    "    module_path, class_name = target.rsplit('.', 1)\n",
    "    module = importlib.import_module(module_path)\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "\n",
    "def instantiate_from_config(conf):\n",
    "    \"\"\"Instantiate an object from a config dict with `class_path` and `init_args`.\"\"\"\n",
    "    if conf is None:\n",
    "        return None\n",
    "    target_cls = _resolve_target(conf[\"class_path\"])\n",
    "    kwargs = conf.get(\"init_args\", {}) or {}\n",
    "    return target_cls(**kwargs)\n",
    "\n",
    "\n",
    "def callable_from_config(conf):\n",
    "    \"\"\"Create a callable (for optimizer/scheduler) that defers parameter binding.\"\"\"\n",
    "    if conf is None:\n",
    "        return None\n",
    "    target_cls = _resolve_target(conf[\"class_path\"])\n",
    "    kwargs = conf.get(\"init_args\", {}) or {}\n",
    "    return lambda *args, **extra: target_cls(*args, **{**kwargs, **extra})\n",
    "\n",
    "\n",
    "def list_from_configs(confs):\n",
    "    if confs is None:\n",
    "        return None\n",
    "    return [instantiate_from_config(c) for c in confs]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --- Configure paths and training hyperparameters ---\n",
    "IMAGENET_ROOT = \"/path/to/imagenet/train\"          # ImageNet training folder\n",
    "DINOV2_WEIGHTS = \"/path/to/dinov2_vitb14\"          # Local DINOv2 ViT-B/14 checkpoint directory\n",
    "WORKDIR_ROOT = Path(\"./workdirs_notebook\")          # Where logs, checkpoints, and samples will be saved\n",
    "\n",
    "EXP_NAME = f\"pixnerd_imagenet256_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "BASE_CONFIG = Path(\"configs_c2i/pix256std1_repa_pixnerd_xl.yaml\")\n",
    "\n",
    "cfg = OmegaConf.load(BASE_CONFIG)\n",
    "\n",
    "# Override training/logging paths to local locations\n",
    "cfg.tags.exp = EXP_NAME\n",
    "cfg.trainer.default_root_dir = str(WORKDIR_ROOT)\n",
    "if cfg.trainer.logger:\n",
    "    cfg.trainer.logger.init_args.name = EXP_NAME\n",
    "    cfg.trainer.logger.init_args.project = cfg.trainer.logger.init_args.get(\"project\", \"universal_pix_flow\")\n",
    "\n",
    "# Point datasets and frozen encoder to your local data\n",
    "cfg.data.train_dataset.init_args.root = IMAGENET_ROOT\n",
    "cfg.data.train_dataset.init_args.resolution = 256\n",
    "cfg.data.eval_dataset.init_args.latent_shape = [3, 256, 256]\n",
    "cfg.data.pred_dataset.init_args.latent_shape = [3, 256, 256]\n",
    "cfg.diffusion_trainer.init_args.encoder.init_args.weight_path = DINOV2_WEIGHTS\n",
    "\n",
    "# Cache overrides are optional; keep them None unless you need a custom hub path\n",
    "cfg.torch_hub_dir = None\n",
    "cfg.huggingface_cache_dir = None\n",
    "\n",
    "# Save the run-specific config next to this notebook\n",
    "notebook_cfg = Path(\"notebooks/configs/pixnerd_imagenet256_notebook.yaml\")\n",
    "notebook_cfg.parent.mkdir(parents=True, exist_ok=True)\n",
    "OmegaConf.save(cfg, notebook_cfg)\n",
    "print(\"Saved config to\", notebook_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved config mirrors `configs_c2i/pix256std1_repa_pixnerd_xl.yaml` but with local paths/tags. Training outputs land in `WORKDIR_ROOT/exp_<EXP_NAME>/...`, and the `SaveImagesHook` writes validation/prediction images under the same root."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: peek at the resolved config\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Lightning objects without the CLI\n",
    "This cell constructs the datamodule, model parts, callbacks, and trainer directly from the config so you can run training entirely inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Respect optional hub caches\n",
    "if cfg.get(\"huggingface_cache_dir\"):\n",
    "    os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cfg.huggingface_cache_dir\n",
    "if cfg.get(\"torch_hub_dir\"):\n",
    "    os.environ[\"TORCH_HOME\"] = cfg.torch_hub_dir\n",
    "    torch.hub.set_dir(cfg.torch_hub_dir)\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# Instantiate datasets and datamodule\n",
    "train_dataset = instantiate_from_config(OmegaConf.to_container(cfg.data.train_dataset, resolve=True))\n",
    "eval_dataset = instantiate_from_config(OmegaConf.to_container(cfg.data.eval_dataset, resolve=True))\n",
    "pred_dataset = instantiate_from_config(OmegaConf.to_container(cfg.data.pred_dataset, resolve=True))\n",
    "\n",
    "data_module = DataModule(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    pred_dataset=pred_dataset,\n",
    "    train_batch_size=cfg.data.train_batch_size,\n",
    "    train_num_workers=cfg.data.train_num_workers,\n",
    "    pred_batch_size=cfg.data.pred_batch_size,\n",
    "    pred_num_workers=cfg.data.pred_num_workers,\n",
    ")\n",
    "\n",
    "# Instantiate model components\n",
    "vae = instantiate_from_config(OmegaConf.to_container(cfg.model.vae, resolve=True))\n",
    "conditioner = instantiate_from_config(OmegaConf.to_container(cfg.model.conditioner, resolve=True))\n",
    "denoiser = instantiate_from_config(OmegaConf.to_container(cfg.model.denoiser, resolve=True))\n",
    "diffusion_trainer = instantiate_from_config(OmegaConf.to_container(cfg.diffusion_trainer, resolve=True))\n",
    "diffusion_sampler = instantiate_from_config(OmegaConf.to_container(cfg.diffusion_sampler, resolve=True))\n",
    "ema_tracker = instantiate_from_config(OmegaConf.to_container(cfg.ema_tracker, resolve=True))\n",
    "optimizer_fn = callable_from_config(OmegaConf.to_container(cfg.optimizer, resolve=True))\n",
    "lr_scheduler_fn = callable_from_config(OmegaConf.to_container(cfg.get(\"lr_scheduler\"), resolve=True))\n",
    "\n",
    "lightning_model = LightningModel(\n",
    "    vae=vae,\n",
    "    conditioner=conditioner,\n",
    "    denoiser=denoiser,\n",
    "    diffusion_trainer=diffusion_trainer,\n",
    "    diffusion_sampler=diffusion_sampler,\n",
    "    ema_tracker=ema_tracker,\n",
    "    optimizer=optimizer_fn,\n",
    "    lr_scheduler=lr_scheduler_fn,\n",
    ")\n",
    "\n",
    "# Prepare EMA copies and compiled models\n",
    "lightning_model.configure_model()\n",
    "\n",
    "# Build callbacks/logger/plugins\n",
    "trainer_conf = OmegaConf.to_container(cfg.trainer, resolve=True)\n",
    "logger_conf = trainer_conf.pop(\"logger\", None)\n",
    "callbacks_conf = trainer_conf.pop(\"callbacks\", [])\n",
    "plugins_conf = trainer_conf.pop(\"plugins\", None)\n",
    "\n",
    "logger = instantiate_from_config(logger_conf) if logger_conf else None\n",
    "callbacks = list_from_configs(callbacks_conf) or None\n",
    "plugins = list_from_configs(plugins_conf) if plugins_conf else None\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    plugins=plugins,\n",
    "    **trainer_conf,\n",
    ")\n",
    "\n",
    "run_dir = Path(trainer.default_root_dir) / f\"exp_{cfg.tags.exp}\"\n",
    "print(\"Trainer default_root_dir:\", trainer.default_root_dir)\n",
    "print(\"Run directory:\", run_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch training\n",
    "This runs standard PixNerd ImageNet training directly via `trainer.fit`. Expect the same runtime and logging behavior as the CLI version."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Uncomment to start training\n",
    "# trainer.fit(lightning_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, checkpoints appear under `run_dir / \"checkpoints\"`. Point `CKPT_PATH` at the checkpoint you want to sample."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "CKPT_PATH = run_dir / \"checkpoints/last.ckpt\"  # update if you prefer a specific step\n",
    "print(\"Expected checkpoint:\", CKPT_PATH)\n",
    "\n",
    "# Utility to load a checkpoint back into the in-memory model (or skip if you just finished training)\n",
    "if CKPT_PATH.exists():\n",
    "    state = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "    lightning_model.load_state_dict(state[\"state_dict\"], strict=False)\n",
    "    print(\"Loaded weights from\", CKPT_PATH)\n",
    "else:\n",
    "    print(\"Checkpoint not found yet \u2014 train first or update CKPT_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample at training resolution (256\u00d7256)\n",
    "Use `trainer.predict` with the EMA weights to generate class-conditional samples at the training size. Outputs are written by `SaveImagesHook` under the run directory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Uncomment to generate samples\n",
    "# trainer.predict(lightning_model, dataloaders=data_module.predict_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-resolution sampling (e.g., 512\u00d7512)\n",
    "Clone the saved config, bump the latent shape to the target resolution, lower the batch size if needed, rebuild the datamodule/model, and run `trainer.predict` again."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "superres_cfg = OmegaConf.load(notebook_cfg)\n",
    "superres_cfg.data.pred_dataset.init_args.latent_shape = [3, 512, 512]\n",
    "superres_cfg.data.pred_batch_size = 4  # adjust for your memory budget\n",
    "superres_cfg.trainer.default_root_dir = str(Path(cfg.trainer.default_root_dir) / \"superres_runs\")\n",
    "\n",
    "superres_cfg_path = Path(\"notebooks/configs/pixnerd_imagenet512_predict.yaml\")\n",
    "OmegaConf.save(superres_cfg, superres_cfg_path)\n",
    "print(\"Super-resolution config saved to\", superres_cfg_path)\n",
    "\n",
    "# Build fresh prediction objects\n",
    "sr_pred_dataset = instantiate_from_config(OmegaConf.to_container(superres_cfg.data.pred_dataset, resolve=True))\n",
    "sr_data_module = DataModule(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    pred_dataset=sr_pred_dataset,\n",
    "    train_batch_size=cfg.data.train_batch_size,\n",
    "    train_num_workers=cfg.data.train_num_workers,\n",
    "    pred_batch_size=superres_cfg.data.pred_batch_size,\n",
    "    pred_num_workers=superres_cfg.data.pred_num_workers,\n",
    ")\n",
    "\n",
    "sr_trainer_conf = OmegaConf.to_container(superres_cfg.trainer, resolve=True)\n",
    "sr_logger_conf = sr_trainer_conf.pop(\"logger\", None)\n",
    "sr_callbacks_conf = sr_trainer_conf.pop(\"callbacks\", [])\n",
    "sr_plugins_conf = sr_trainer_conf.pop(\"plugins\", None)\n",
    "\n",
    "sr_logger = instantiate_from_config(sr_logger_conf) if sr_logger_conf else None\n",
    "sr_callbacks = list_from_configs(sr_callbacks_conf) or None\n",
    "sr_plugins = list_from_configs(sr_plugins_conf) if sr_plugins_conf else None\n",
    "\n",
    "sr_trainer = Trainer(logger=sr_logger, callbacks=sr_callbacks, plugins=sr_plugins, **sr_trainer_conf)\n",
    "print(\"Super-res default_root_dir:\", sr_trainer.default_root_dir)\n",
    "\n",
    "# Uncomment to generate 512x512 (or higher) samples\n",
    "# sr_trainer.predict(lightning_model, dataloaders=sr_data_module.predict_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predicted images\n",
    "`SaveImagesHook` writes PNGs (first few samples) and a compressed `.npz` array. Update `PRED_DIR` to point at the prediction folder for either the 256\u00d7256 or super-resolution run and visualize a handful of images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Point this to the prediction directory you want to browse\n",
    "PRED_DIR = Path(run_dir) / \"val\" / \"predict\"  # adjust if you used a different save_dir\n",
    "print(\"Reading from\", PRED_DIR)\n",
    "\n",
    "images = sorted(PRED_DIR.glob(\"*.png\"))[:8]\n",
    "if not images:\n",
    "    print(\"No PNGs found yet. Run predict first or point PRED_DIR to your output folder.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(4*len(images), 4))\n",
    "    for ax, image_path in zip(axes, images):\n",
    "        ax.imshow(Image.open(image_path))\n",
    "        ax.set_title(image_path.name)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}