{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# PixNerd checkpoint inference (ImageNet class-conditional)\n\nThis notebook loads a trained PixNerd XL checkpoint, runs class-conditional sampling, and visualizes both 256\u00d7256 and higher-resolution (e.g., 512\u00d7512) outputs without relying on the Lightning CLI. Configure the checkpoint path and a few options at the top, then run the cells sequentially on a GPU runtime.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Environment & paths\n- Assumes the repository is already installed (dependencies from `requirements.txt`).\n- Provide the checkpoint at `checkpoints/PixNerd-XL-P16-C2I/epoch=319-step=1600000.ckpt` relative to the repo root (adjustable below).\n- Sampling expects a CUDA GPU; the sampler uses bfloat16 autocast on CUDA.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nfrom pathlib import Path\nimport math\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Optional: change to your repo root if running from elsewhere\nREPO_ROOT = Path(os.getcwd())\n\n# Checkpoint location (adjust if yours lives elsewhere)\nCKPT_PATH = REPO_ROOT / \"checkpoints/PixNerd-XL-P16-C2I/epoch=319-step=1600000.ckpt\"\n\n# Where to save generated images\nOUTPUT_DIR = REPO_ROOT / \"notebooks\" / \"pixnerd_inference_outputs\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Repo root: {REPO_ROOT}\")\nprint(f\"Checkpoint exists: {CKPT_PATH.exists()}\")\nprint(f\"Using device: {DEVICE}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Build the PixNerd XL model components\nThis mirrors the training configuration used for ImageNet 256\u00d7256:\n- Pixel-space autoencoder (`PixelAE`)\n- Label conditioner (1000 ImageNet classes)\n- PixNerDiT XL denoiser (patch size 16, 30 blocks)\n- Flow-matching Euler sampler with classifier-free guidance\n- REPATrainer stub (only needed to load projection weights from the checkpoint)\n- SimpleEMA tracker to hold EMA weights from training\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from src.models.autoencoder.pixel import PixelAE\nfrom src.models.conditioner.class_label import LabelConditioner\nfrom src.models.transformer.pixnerd_c2i import PixNerDiT\nfrom src.diffusion.flow_matching.scheduling import LinearScheduler\nfrom src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\nfrom src.diffusion.base.guidance import simple_guidance_fn\nfrom src.diffusion.flow_matching.training_repa import REPATrainer\nfrom src.callbacks.simple_ema import SimpleEMA\nfrom src.lightning_model import LightningModel\nfrom src.models.encoder import IndentityMapping\nfrom src.models.autoencoder.base import fp2uint8\n\n# Model hyperparameters copied from configs_c2i/pix256std1_repa_pixnerd_xl.yaml\nHIDDEN_SIZE = 1152\nNUM_CLASSES = 1000\nPATCH_SIZE = 16\n\n# Schedulers\nmain_scheduler = LinearScheduler()\nguide_scheduler = LinearScheduler()\n\n# Core modules\nvae = PixelAE(scale=1.0)\nconditioner = LabelConditioner(num_classes=NUM_CLASSES)\ndenoiser = PixNerDiT(\n    in_channels=3,\n    patch_size=PATCH_SIZE,\n    num_groups=16,\n    hidden_size=HIDDEN_SIZE,\n    hidden_size_x=64,\n    num_blocks=30,\n    num_cond_blocks=26,\n    nerf_mlpratio=2,\n    num_classes=NUM_CLASSES,\n)\n\n# Sampler mirrors the ImageNet setup\nsampler = EulerSampler(\n    num_steps=100,\n    guidance=3.5,\n    guidance_interval_min=0.1,\n    guidance_interval_max=1.0,\n    scheduler=main_scheduler,\n    w_scheduler=guide_scheduler,\n    guidance_fn=simple_guidance_fn,\n    step_fn=ode_step_fn,\n)\n\n# REPATrainer is only instantiated so its projection MLP weights can be loaded from the checkpoint\ntrainer_stub = REPATrainer(\n    scheduler=main_scheduler,\n    lognorm_t=True,\n    encoder=IndentityMapping(),  # placeholder; encoder is unused for inference\n    align_layer=8,\n    proj_denoiser_dim=HIDDEN_SIZE,\n    proj_hidden_dim=HIDDEN_SIZE,\n    proj_encoder_dim=768,\n)\n\nema_tracker = SimpleEMA(decay=0.9999)\n\n# Wrap everything in the LightningModel for easy checkpoint loading\nmodel = LightningModel(\n    vae=vae,\n    conditioner=conditioner,\n    denoiser=denoiser,\n    diffusion_trainer=trainer_stub,\n    diffusion_sampler=sampler,\n    ema_tracker=ema_tracker,\n    optimizer=None,\n    lr_scheduler=None,\n    eval_original_model=False,\n)\nmodel.eval()\nmodel.to(DEVICE)\nprint(\"Model initialized.\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Load the checkpoint (EMA weights)\n`strict=False` lets us ignore any optimizer or trainer state that may be absent. EMA weights are stored under `ema_denoiser.*` and are used for sampling.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\nmissing, unexpected = model.load_state_dict(ckpt[\"state_dict\"], strict=False)\nprint(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\nprint(\"Ready for sampling with EMA denoiser.\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Sampling helpers\n- `sample_imagenet_classes` draws Gaussian noise, runs the Euler sampler, decodes to pixel space, and returns uint8 tensors.\n- `save_grid` writes a simple grid PNG for quick inspection.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@torch.no_grad()\ndef sample_imagenet_classes(class_ids, height=256, width=256, seed=0, guidance=3.5, num_steps=100):\n    torch.manual_seed(seed)\n    batch = len(class_ids)\n    # Update guidance or steps on the fly if desired\n    model.diffusion_sampler.guidance = guidance\n    model.diffusion_sampler.num_steps = num_steps\n\n    noise = torch.randn(batch, 3, height, width, device=DEVICE)\n    labels = torch.tensor(class_ids, device=DEVICE)\n\n    condition, uncondition = model.conditioner(labels)\n    samples = model.diffusion_sampler(\n        model.ema_denoiser,\n        noise,\n        condition,\n        uncondition,\n    )\n    images = model.vae.decode(samples)\n    images = torch.clamp(images, -1.0, 1.0)\n    images_uint8 = fp2uint8(images)\n    return images_uint8.cpu()\n\n\ndef save_grid(images_uint8, filename, cols=None):\n    imgs = [Image.fromarray(img) for img in images_uint8]\n    n = len(imgs)\n    if cols is None:\n        cols = math.ceil(math.sqrt(n))\n    rows = math.ceil(n / cols)\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", (cols * w, rows * h))\n    for idx, img in enumerate(imgs):\n        r, c = divmod(idx, cols)\n        grid.paste(img, (c * w, r * h))\n    out_path = OUTPUT_DIR / filename\n    grid.save(out_path)\n    return out_path\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Generate 256\u00d7256 ImageNet samples\nChoose a few class IDs (0\u2013999). The example below uses three diverse classes. Adjust `SEED`, `GUIDANCE`, or `NUM_STEPS` as needed.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "CLASS_IDS = [207, 130, 340]  # golden retriever, flamingo, zebra (example ImageNet IDs)\nSEED = 42\nGUIDANCE = 3.5\nNUM_STEPS = 100\n\nimages_256 = sample_imagenet_classes(\n    class_ids=CLASS_IDS,\n    height=256,\n    width=256,\n    seed=SEED,\n    guidance=GUIDANCE,\n    num_steps=NUM_STEPS,\n)\n\nprint(f\"Generated batch shape: {images_256.shape}\")\n_ = plt.figure(figsize=(12, 4))\nplt.imshow(Image.fromarray(images_256[0]))\nplt.axis('off')\nplt.show()\n\nsave_path_256 = save_grid(images_256, \"samples_256.png\")\nprint(f\"Saved grid to {save_path_256}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Super-resolution sampling (e.g., 512\u00d7512)\nBecause the model works in pixel space with coordinate-aware embeddings, simply draw higher-resolution noise and run the same sampler. You can tweak guidance or steps separately for super-res runs.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "HIGH_RES = 512  # set to any square size supported by your GPU\nSUPERRES_SEED = 7\nSUPERRES_GUIDANCE = 4.0\nSUPERRES_STEPS = 120\n\nimages_sr = sample_imagenet_classes(\n    class_ids=CLASS_IDS,\n    height=HIGH_RES,\n    width=HIGH_RES,\n    seed=SUPERRES_SEED,\n    guidance=SUPERRES_GUIDANCE,\n    num_steps=SUPERRES_STEPS,\n)\n\nprint(f\"Super-res batch shape: {images_sr.shape}\")\n_ = plt.figure(figsize=(12, 4))\nplt.imshow(Image.fromarray(images_sr[0]))\nplt.axis('off')\nplt.show()\n\nsave_path_sr = save_grid(images_sr, f\"samples_{HIGH_RES}.png\")\nprint(f\"Saved grid to {save_path_sr}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notes\n- If you see autocast errors on CPU, switch to a GPU runtime; the sampler decorates its forward pass with CUDA autocast.\n- To reproduce training-time sampler behavior, keep `guidance_interval_min=0.1`, `guidance_interval_max=1.0`, and `num_steps=100`.\n- For deterministic reruns, fix `seed` and the class list; GPU algorithms may still introduce minor nondeterminism.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}